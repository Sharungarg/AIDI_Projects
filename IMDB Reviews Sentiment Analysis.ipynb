{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01abacdc",
   "metadata": {},
   "source": [
    "# IMDb Movie Review - Project 3\n",
    "> Name: Sharun Garg  \n",
    "> Student ID: 200493338"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb4269",
   "metadata": {},
   "source": [
    "# 1. Importing important libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25918762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0666b",
   "metadata": {},
   "source": [
    "### Importing Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b22879",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.6.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\r\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72af60",
   "metadata": {},
   "source": [
    "# 2. Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb4ae53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"IMDB_dataset.xlsx\"\n",
    "data = pd.read_excel(file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcf3dc",
   "metadata": {},
   "source": [
    "> We can see that there are two columns in the data set with **review** columns having the text data and **sentiment** column giving indication if it is a positive or negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767b3f4",
   "metadata": {},
   "source": [
    "### More info on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4487bd61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 25000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data has {} rows and {} columns\".format(len(data), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5c021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 25000 reviews, 12500 are positive, 12500 are negative\n"
     ]
    }
   ],
   "source": [
    "print(\"Out of {} reviews, {} are positive, {} are negative\".format(len(data),\n",
    "                                                       len(data[data['sentiment']=='positive']),\n",
    "                                                       len(data[data['sentiment']=='negative'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363d3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in label: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null in label: {}\".format(data['sentiment'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(data['review'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0576be7",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54528a3d",
   "metadata": {},
   "source": [
    "## Removing the punctuations in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99942149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  positive   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  negative   \n",
       "\n",
       "                                                                                     review_text_clean  \n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...  \n",
       "1  Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...  \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...  \n",
       "3  This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...  \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['review_text_clean'] = data['review'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f8d8e",
   "metadata": {},
   "source": [
    "> All the punctuation marks are removed from the reviews as part of data cleaning and the text is stored in new column **review_text_clean**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edc0a0",
   "metadata": {},
   "source": [
    "## Tokenizing the text data into separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678ed0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text_clean</th>\n",
       "      <th>review_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to, spend, time, on, a, too, hot, summer, weekend, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, story, of, selflessness, sacrifice, and, dedication,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrection, of, a, up, dated, seahunt, series, with, the, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...</td>\n",
       "      <td>[this, show, was, an, amazing, fresh, innovative, idea, in, the, 70s, when, it, first, aired, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>[encouraged, by, the, positive, comments, about, this, film, on, here, i, was, looking, forward,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  positive   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  negative   \n",
       "\n",
       "                                                                                     review_text_clean  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...   \n",
       "1  Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "                                                                                 review_text_tokenized  \n",
       "0  [i, thought, this, was, a, wonderful, way, to, spend, time, on, a, too, hot, summer, weekend, si...  \n",
       "1  [probably, my, alltime, favorite, movie, a, story, of, selflessness, sacrifice, and, dedication,...  \n",
       "2  [i, sure, would, like, to, see, a, resurrection, of, a, up, dated, seahunt, series, with, the, t...  \n",
       "3  [this, show, was, an, amazing, fresh, innovative, idea, in, the, 70s, when, it, first, aired, th...  \n",
       "4  [encouraged, by, the, positive, comments, about, this, film, on, here, i, was, looking, forward,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['review_text_tokenized'] = data['review_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693fe1a1",
   "metadata": {},
   "source": [
    "> Tokenization process is applied on **review_text_clean** (text without punctuations) and the text is converted into an array of separate tokens and stored in **review_text_tokenized**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77eff7",
   "metadata": {},
   "source": [
    "##  Removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f3024a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text_clean</th>\n",
       "      <th>review_text_tokenized</th>\n",
       "      <th>review_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to, spend, time, on, a, too, hot, summer, weekend, si...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, story, of, selflessness, sacrifice, and, dedication,...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrection, of, a, up, dated, seahunt, series, with, the, t...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...</td>\n",
       "      <td>[this, show, was, an, amazing, fresh, innovative, idea, in, the, 70s, when, it, first, aired, th...</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>[encouraged, by, the, positive, comments, about, this, film, on, here, i, was, looking, forward,...</td>\n",
       "      <td>[encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  positive   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  negative   \n",
       "\n",
       "                                                                                     review_text_clean  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air ...   \n",
       "1  Probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble c...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing fresh  innovative idea in the 70s when it first aired The first 7 or 8 ...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "                                                                                 review_text_tokenized  \\\n",
       "0  [i, thought, this, was, a, wonderful, way, to, spend, time, on, a, too, hot, summer, weekend, si...   \n",
       "1  [probably, my, alltime, favorite, movie, a, story, of, selflessness, sacrifice, and, dedication,...   \n",
       "2  [i, sure, would, like, to, see, a, resurrection, of, a, up, dated, seahunt, series, with, the, t...   \n",
       "3  [this, show, was, an, amazing, fresh, innovative, idea, in, the, 70s, when, it, first, aired, th...   \n",
       "4  [encouraged, by, the, positive, comments, about, this, film, on, here, i, was, looking, forward,...   \n",
       "\n",
       "                                                                                    review_text_nostop  \n",
       "0  [thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...  \n",
       "1  [probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...  \n",
       "2  [sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...  \n",
       "3  [show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...  \n",
       "4  [encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "data['review_text_nostop'] = data['review_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747cad0",
   "metadata": {},
   "source": [
    "> The stopwords are a list of words that are very very common but donâ€™t provide useful information for most text analysis procedures. So they are removed before analysis to reduce the overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1577955",
   "metadata": {},
   "source": [
    "## Cleaning process Before and After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77eaee1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "\n",
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  positive   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  negative   \n",
       "\n",
       "                                                                                    review_text_nostop  \n",
       "0  [thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...  \n",
       "1  [probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...  \n",
       "2  [sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...  \n",
       "3  [show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...  \n",
       "4  [encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['review_text_clean', 'review_text_tokenized'], axis = 1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685921ed",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "> Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae35fab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text_nostop</th>\n",
       "      <th>review_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, first, aired, first, 7, 8, year, brilliant, thing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...</td>\n",
       "      <td>[encouraged, positive, comment, film, looking, forward, watching, film, bad, mistake, ive, seen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phil the Alien is one of those quirky films where the humour is based around the oddness of ever...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[phil, alien, one, quirky, films, humour, based, around, oddness, everything, rather, actual, pu...</td>\n",
       "      <td>[phil, alien, one, quirky, film, humour, based, around, oddness, everything, rather, actual, pun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I saw this movie when I was about 12 when it came out. I recall the scariest scene was the big b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[saw, movie, 12, came, recall, scariest, scene, big, bird, eating, men, dangling, helplessly, pa...</td>\n",
       "      <td>[saw, movie, 12, came, recall, scariest, scene, big, bird, eating, men, dangling, helplessly, pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "1  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a nobl...   \n",
       "2  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have tod...   \n",
       "3  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 o...   \n",
       "4  Encouraged by the positive comments about this film on here I was looking forward to watching th...   \n",
       "5  Phil the Alien is one of those quirky films where the humour is based around the oddness of ever...   \n",
       "6  I saw this movie when I was about 12 when it came out. I recall the scariest scene was the big b...   \n",
       "\n",
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  positive   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  negative   \n",
       "5  negative   \n",
       "6  negative   \n",
       "\n",
       "                                                                                    review_text_nostop  \\\n",
       "0  [thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...   \n",
       "1  [probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...   \n",
       "2  [sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...   \n",
       "3  [show, amazing, fresh, innovative, idea, 70s, first, aired, first, 7, 8, years, brilliant, thing...   \n",
       "4  [encouraged, positive, comments, film, looking, forward, watching, film, bad, mistake, ive, seen...   \n",
       "5  [phil, alien, one, quirky, films, humour, based, around, oddness, everything, rather, actual, pu...   \n",
       "6  [saw, movie, 12, came, recall, scariest, scene, big, bird, eating, men, dangling, helplessly, pa...   \n",
       "\n",
       "                                                                                review_text_lemmatized  \n",
       "0  [thought, wonderful, way, spend, time, hot, summer, weekend, sitting, air, conditioned, theater,...  \n",
       "1  [probably, alltime, favorite, movie, story, selflessness, sacrifice, dedication, noble, cause, p...  \n",
       "2  [sure, would, like, see, resurrection, dated, seahunt, series, tech, today, would, bring, back, ...  \n",
       "3  [show, amazing, fresh, innovative, idea, 70, first, aired, first, 7, 8, year, brilliant, thing, ...  \n",
       "4  [encouraged, positive, comment, film, looking, forward, watching, film, bad, mistake, ive, seen,...  \n",
       "5  [phil, alien, one, quirky, film, humour, based, around, oddness, everything, rather, actual, pun...  \n",
       "6  [saw, movie, 12, came, recall, scariest, scene, big, bird, eating, men, dangling, helplessly, pa...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "data['review_text_lemmatized'] = data['review_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790aea4",
   "metadata": {},
   "source": [
    "# 4. TF-IDF Vectorization and Data Processing Pipeline\n",
    "> Performing the process of removing punctuations, removing stopwords and doing tokenization on the data.  \n",
    "After that TF-IDF Vectorization is applied to the cleaned data for converting the text into vectors for performing the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93cf666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "data = pd.read_excel(file)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3444b0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 114083)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bf3de",
   "metadata": {},
   "source": [
    "> It can be observed that TF-IDF vectorization converted the cleaned tokenized texts into vectors spead over 114083 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016955fd",
   "metadata": {},
   "source": [
    "# 5. Classification using RandomForest and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac27951",
   "metadata": {},
   "source": [
    "**Separating the sentiment column as target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9910f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = data['sentiment']\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb3e72",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning of Random Forest Classifier using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e1cf185",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.718268</td>\n",
       "      <td>0.183859</td>\n",
       "      <td>0.324236</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 100}</td>\n",
       "      <td>0.8464</td>\n",
       "      <td>0.8562</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>0.84988</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67.319619</td>\n",
       "      <td>0.984776</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 100}</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>0.8442</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.84940</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.498685</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>0.248001</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 100}</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.8486</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>0.84056</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33.439558</td>\n",
       "      <td>0.230993</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 50}</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>0.8410</td>\n",
       "      <td>0.8326</td>\n",
       "      <td>0.8362</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.83304</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.864103</td>\n",
       "      <td>0.070747</td>\n",
       "      <td>0.183527</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 50}</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>0.8366</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.8278</td>\n",
       "      <td>0.83240</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5      33.718268      0.183859         0.324236        0.002039   \n",
       "8      67.319619      0.984776         0.450469        0.017627   \n",
       "2      15.498685      0.073754         0.248001        0.001774   \n",
       "7      33.439558      0.230993         0.238960        0.001536   \n",
       "4      16.864103      0.070747         0.183527        0.002084   \n",
       "\n",
       "  param_max_depth param_n_estimators  \\\n",
       "5              50                100   \n",
       "8            None                100   \n",
       "2              25                100   \n",
       "7            None                 50   \n",
       "4              50                 50   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "5    {'max_depth': 50, 'n_estimators': 100}             0.8464   \n",
       "8  {'max_depth': None, 'n_estimators': 100}             0.8522   \n",
       "2    {'max_depth': 25, 'n_estimators': 100}             0.8380   \n",
       "7   {'max_depth': None, 'n_estimators': 50}             0.8294   \n",
       "4     {'max_depth': 50, 'n_estimators': 50}             0.8288   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "5             0.8562             0.8532             0.8478             0.8458   \n",
       "8             0.8594             0.8458             0.8442             0.8454   \n",
       "2             0.8486             0.8372             0.8402             0.8388   \n",
       "7             0.8410             0.8326             0.8362             0.8260   \n",
       "4             0.8366             0.8316             0.8372             0.8278   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "5          0.84988        0.004100                1  \n",
       "8          0.84940        0.005724                2  \n",
       "2          0.84056        0.004140                3  \n",
       "7          0.83304        0.005222                4  \n",
       "4          0.83240        0.003884                5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [25, 50, 100],\n",
    "        'max_depth': [25, 50, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=2)\n",
    "gs_fit = gs.fit(X_tfidf, target)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538b59e",
   "metadata": {},
   "source": [
    "> It can be observed that the parameters 'max_depth' of 50 and 'n_estimators' of 100 had the highest mean accuracy of about **85 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2aa76",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning of Xtreme Gradient Boosting Classifier using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aaded7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_eval_metric</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46.615969</td>\n",
       "      <td>0.594961</td>\n",
       "      <td>0.486483</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eval_metric': 'rmse', 'learning_rate': 1, 'n_estimators': 100}</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.8348</td>\n",
       "      <td>0.8362</td>\n",
       "      <td>0.8394</td>\n",
       "      <td>0.83728</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.742249</td>\n",
       "      <td>4.128671</td>\n",
       "      <td>0.438250</td>\n",
       "      <td>0.057772</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eval_metric': 'mlogloss', 'learning_rate': 1, 'n_estimators': 100}</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.8348</td>\n",
       "      <td>0.8362</td>\n",
       "      <td>0.8394</td>\n",
       "      <td>0.83728</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.185407</td>\n",
       "      <td>0.314176</td>\n",
       "      <td>0.458525</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'eval_metric': 'rmse', 'learning_rate': 1, 'n_estimators': 50}</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8282</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.82984</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.724000</td>\n",
       "      <td>0.936218</td>\n",
       "      <td>0.406835</td>\n",
       "      <td>0.070350</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'eval_metric': 'mlogloss', 'learning_rate': 1, 'n_estimators': 50}</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8282</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.82984</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.896452</td>\n",
       "      <td>0.400030</td>\n",
       "      <td>0.496066</td>\n",
       "      <td>0.035706</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eval_metric': 'rmse', 'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8310</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.82984</td>\n",
       "      <td>0.005622</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5       46.615969      0.594961         0.486483        0.020953   \n",
       "11      44.742249      4.128671         0.438250        0.057772   \n",
       "4       24.185407      0.314176         0.458525        0.023487   \n",
       "10      23.724000      0.936218         0.406835        0.070350   \n",
       "2       48.896452      0.400030         0.496066        0.035706   \n",
       "\n",
       "   param_eval_metric param_learning_rate param_n_estimators  \\\n",
       "5               rmse                   1                100   \n",
       "11          mlogloss                   1                100   \n",
       "4               rmse                   1                 50   \n",
       "10          mlogloss                   1                 50   \n",
       "2               rmse                 0.1                100   \n",
       "\n",
       "                                                                  params  \\\n",
       "5       {'eval_metric': 'rmse', 'learning_rate': 1, 'n_estimators': 100}   \n",
       "11  {'eval_metric': 'mlogloss', 'learning_rate': 1, 'n_estimators': 100}   \n",
       "4        {'eval_metric': 'rmse', 'learning_rate': 1, 'n_estimators': 50}   \n",
       "10   {'eval_metric': 'mlogloss', 'learning_rate': 1, 'n_estimators': 50}   \n",
       "2     {'eval_metric': 'rmse', 'learning_rate': 0.1, 'n_estimators': 100}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "5              0.8396             0.8364             0.8348   \n",
       "11             0.8396             0.8364             0.8348   \n",
       "4              0.8276             0.8276             0.8282   \n",
       "10             0.8276             0.8276             0.8282   \n",
       "2              0.8260             0.8402             0.8276   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "5              0.8362             0.8394          0.83728        0.001896   \n",
       "11             0.8362             0.8394          0.83728        0.001896   \n",
       "4              0.8342             0.8316          0.82984        0.002639   \n",
       "10             0.8342             0.8316          0.82984        0.002639   \n",
       "2              0.8310             0.8244          0.82984        0.005622   \n",
       "\n",
       "    rank_test_score  \n",
       "5                 1  \n",
       "11                1  \n",
       "4                 3  \n",
       "10                3  \n",
       "2                 5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "param = {\n",
    "    'n_estimators': [25, 50, 100], \n",
    "    'eval_metric': ['rmse', 'mlogloss'],\n",
    "    'learning_rate': [0.1, 1]\n",
    "}\n",
    "\n",
    "\n",
    "gs_xgb = GridSearchCV(xgb, param, cv=5, n_jobs=2)\n",
    "cv_fit = gs_xgb.fit(X_tfidf, target)\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84ddf5",
   "metadata": {},
   "source": [
    "> It can be observed that the parameters 'n_estimator' of 100, 'learning_rate' of 1, and 'eval_metric' as 'rmse' had the highest mean accuracy of **83.7 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c13d1",
   "metadata": {},
   "source": [
    "## Splitting the data into test and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b4f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in training set: 20000 \n",
      "No. of rows in test set: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of rows in training set: {} \\nNo. of rows in test set: {}\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122ad93",
   "metadata": {},
   "source": [
    "## Creating RandomForestClassifier with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72852b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 16.848 / Predict time: 0.175 ---- Precision: 0.841 / Recall: 0.86 / Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=2)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354ee2c",
   "metadata": {},
   "source": [
    "> it can be observed that the RandomForestClassifier had a pretty decent Accuracy of '85%' and a predict time of 175 msecs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799dc05",
   "metadata": {},
   "source": [
    "## Creating XGBoostClassifier with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b7e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 28.759 / Predict time: 0.295 ---- Precision: 0.824 / Recall: 0.852 / Accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(learning_rate= 1, n_estimators= 100, eval_metric= 'rmse')\n",
    "\n",
    "start = time.time()\n",
    "xgb_model = xgb.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = xgb.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69490d2c",
   "metadata": {},
   "source": [
    "> it can be observed that the XtremeGradientBoostingClassifier had a pretty decent Accuracy of '83.7%' but a larger predict time of 295 msecs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e5fb6",
   "metadata": {},
   "source": [
    "# Result:\n",
    "The **RandomForestClassifier** performed better than **XtremeGradientBoostingClassifier** as the fit time was less than about 1.7 times and a faster predict time by about 1.7 times. Randorm Forest Classifier also had a better accuracy of 85% as compared to Extreme Boosting Classifier having accuracy of 83.7%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
